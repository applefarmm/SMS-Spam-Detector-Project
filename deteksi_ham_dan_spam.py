# -*- coding: utf-8 -*-
"""deteksi ham dan spam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vRiHG7H5Tvj_6uSY9_ReYEpzEDFkqALc
"""

# library
import pandas as pd
import numpy as np
from scipy.sparse import csr_matrix
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import cross_val_score

# mengambil data
data = pd.read_csv('/content/drive/MyDrive/kerja praktik/dataset/spam - spam.csv', encoding='latin-1')
data.head(10)

data = data[['Class', 'Message']]

data.head(10)

x = np.array(data["Message"])
y = np.array(data["Class"])
cv = CountVectorizer()

X = cv.fit_transform(x)

# set pelatihan dan set pengujian secara acak
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

deteksi = MultinomialNB()
deteksi.fit(X_train, y_train)

# accuracy machine learning

from sklearn.metrics import accuracy_score
y_pred = deteksi.predict(X_test)
print(accuracy_score(y_test, y_pred))

# prompt: confusion matrix

# Assuming 'y_test' and 'y_pred' are your true and predicted labels, respectively
cm = confusion_matrix(y_test, y_pred)
print(cm)

# analisis data

import matplotlib.pyplot as plt
import seaborn as sns

# Get some descriptive statistics about the data
data.describe()

# Visualize the distribution of spam and ham messages
sns.countplot(x="Class", data=data)
plt.show()

# Analyze the most frequent words in spam and ham messages
spam_words = []
ham_words = []

for message, label in zip(data["Message"], data["Class"]):
  if label == "spam":
    spam_words.extend(message.split())
  else:
    ham_words.extend(message.split())

spam_word_counts = {}
ham_word_counts = {}

for word in spam_words:
  spam_word_counts[word] = spam_word_counts.get(word, 0) + 1

for word in ham_words:
  ham_word_counts[word] = ham_word_counts.get(word, 0) + 1

# Visualize the most frequent words in spam and ham messages
plt.figure(figsize=(10, 6))
sns.barplot(x=list(spam_word_counts.keys())[:10], y=list(spam_word_counts.values())[:10])
plt.title("Most Frequent Words in Spam Messages")
plt.show()

plt.figure(figsize=(10, 6))
sns.barplot(x=list(ham_word_counts.keys())[:10], y=list(ham_word_counts.values())[:10])
plt.title("Most Frequent Words in Ham Messages")
plt.show()

# prompt: wordcloud

import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Create a word cloud for spam messages
spam_wordcloud = WordCloud(width=800, height=600, background_color="white").generate(" ".join(spam_words))
plt.figure(figsize=(10, 6))
plt.imshow(spam_wordcloud, interpolation="bilinear")
plt.axis("off")
plt.title("Word Cloud for Spam Messages")
plt.show()

# Create a word cloud for ham messages
ham_wordcloud = WordCloud(width=800, height=600, background_color="white").generate(" ".join(ham_words))
plt.figure(figsize=(10, 6))
plt.imshow(ham_wordcloud, interpolation="bilinear")
plt.axis("off")
plt.title("Word Cloud for Ham Messages")
plt.show()

# prompt: split the data

# Assuming 'data' is your DataFrame containing 'Message' and 'Class' columns
from sklearn.model_selection import train_test_split

X = data['Message']
y = data['Class']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Adjust test_size and random_state as needed

print("Training data shape:", X_train.shape, y_train.shape)
print("Testing data shape:", X_test.shape, y_test.shape)

# prompt: show class spam only

# Filter the DataFrame to show only spam messages
spam_data = data[data["Class"] == "spam"]

# Display the spam messages
spam_data

# prompt: show class ham only

# Filter the DataFrame to show only ham messages
ham_data = data[data["Class"] == "ham"]

# Display the ham messages
ham_data

# prompt: split ham and spam

ham_messages = data[data['Class'] == 'ham']['Message']
spam_messages = data[data['Class'] == 'spam']['Message']

print("Number of ham messages:", len(ham_messages))
print("Number of spam messages:", len(spam_messages))

# You can further process these separated messages, e.g., for analysis or visualization

# prompt: stemming and lemmatization

import nltk
from nltk.stem import PorterStemmer, WordNetLemmatizer
nltk.download('punkt')
nltk.download('wordnet')

# Initialize stemmer and lemmatizer
stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

# Example text
text = "This is a sentence with running, jumped, and better words."

# Tokenize the text
words = nltk.word_tokenize(text)

# Stemming
stemmed_words = [stemmer.stem(word) for word in words]
print("Stemmed words:", stemmed_words)

# Lemmatization
lemmatized_words = [lemmatizer.lemmatize(word) for word in words]
print("Lemmatized words:", lemmatized_words)

# prompt: Menghapus kata-kata umum (stop words) yang tidak memberikan banyak informasi

nltk.download('stopwords')
from nltk.corpus import stopwords

# ... (previous code)

# Remove stop words
stop_words = set(stopwords.words('english'))  # Customize for your language
filtered_words = [word for word in lemmatized_words if word.lower() not in stop_words]

print("Filtered words (without stop words):", filtered_words)

# prompt: identifikasi dan menganalisis kesalahan prediksi

import numpy as np
# Get the indices of misclassified samples
misclassified_indices = np.where(y_test != y_pred)[0]

# Print some examples of misclassified messages
num_examples_to_print = 5  # Adjust as needed
for i in range(min(num_examples_to_print, len(misclassified_indices))):
  index = misclassified_indices[i]
  true_label = y_test.iloc[index]
  predicted_label = y_pred[index]
  message = X_test.iloc[index]
  print(f"Misclassified example {i+1}:")
  print(f"True label: {true_label} (ham: 0, spam: 1)")
  print(f"Predicted label: {predicted_label}")
  print(f"Message: {message}\n")

# Analyze the types of errors (false positives and false negatives)
false_positives = np.sum((y_test == 0) & (y_pred == 1))
false_negatives = np.sum((y_test == 1) & (y_pred == 0))

print(f"Number of false positives: {false_positives}")
print(f"Number of false negatives: {false_negatives}")

# Optionally, you can further investigate the misclassified samples:
# - Check their length, vocabulary, presence of specific keywords, etc.
# - Try to understand why the model made the wrong prediction.
# - Consider using more advanced techniques like error analysis or explainability tools.

# prompt: mencari pola umum dalam kesalahan untuk perbaikan lebih lanjut

import matplotlib.pyplot as plt
import numpy as np
# Get the indices of misclassified samples
misclassified_indices = np.where(y_test != y_pred)[0]

# Initialize lists to store false positives and false negatives
false_positives = []
false_negatives = []

# Iterate through misclassified samples and categorize them
for index in misclassified_indices:
  true_label = y_test.iloc[index]
  predicted_label = y_pred[index]
  message = X_test.iloc[index]
  if true_label == 0 and predicted_label == 1:  # False positive
    false_positives.append((message, true_label, predicted_label))
  elif true_label == 1 and predicted_label == 0:  # False negative
    false_negatives.append((message, true_label, predicted_label))

# Analyze false positives
print("False Positives:")
for message, true_label, predicted_label in false_positives:
  print(f"Message: {message}")
  # Add analysis of common patterns here, e.g., check for specific words, length, etc.
  print()

# Analyze false negatives
print("\nFalse Negatives:")
for message, true_label, predicted_label in false_negatives:
  print(f"Message: {message}")
  # Add analysis of common patterns here, e.g., check for specific words, length, etc.
  print()

# Example analysis (replace with your specific analysis)
# Check for common words in false positives
fp_words = " ".join([message for message, _, _ in false_positives])
fp_wordcloud = WordCloud().generate(fp_words)
plt.imshow(fp_wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title("Word Cloud for False Positives")
plt.show()

# Check for common words in false negatives
fn_words = " ".join([message for message, _, _ in false_negatives])
fn_wordcloud = WordCloud().generate(fn_words)
plt.imshow(fn_wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title("Word Cloud for False Negatives")
plt.show()

# prompt: input detection of ham and spam

# Load the data (replace with your actual file path)
data = pd.read_csv('/content/drive/MyDrive/kerja praktik/dataset/spam - spam.csv', encoding='latin-1')

# Select relevant columns
data = data[['Class', 'Message']]

# Split data into features (X) and labels (y)
X = data['Message']
y = data['Class'].map({'ham': 0, 'spam': 1})  # Convert labels to numerical

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a CountVectorizer object
cv = CountVectorizer()

# Fit and transform the training data
X_train_counts = cv.fit_transform(X_train)

# Transform the test data
X_test_counts = cv.transform(X_test)

# Train a Multinomial Naive Bayes model
model = MultinomialNB()
model.fit(X_train_counts, y_train)

# Input detection
new_message = input("Enter a message: ")
new_message_counts = cv.transform([new_message])
prediction = model.predict(new_message_counts)

if prediction[0] == 0:
  print("The message is classified as HAM.")
else:
  print("The message is classified as SPAM.")